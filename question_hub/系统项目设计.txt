大概想法：分布式项目，自身做过的项目

Q: CAP 定理
A: Consistency
		一致性，更新操作成功并返回客户端后，所有节点在同一时间的数据完全一致。例如客户端向服务A1写入x，会向A2同步数据，再向服务A2读取该值，获得的也是x，就是一致性。
	Availability
		可用性，服务一直可用，而且是正常响应时间。好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。
	Partition Tolerance 
		分区容错，分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务，不会挂掉。

	为什么不能同时满足：因为网络是不可靠的，如果发生故障导致节点间无法同步数据，那么强一致性和可用性只能二选一。

	CA：不要求分区，则强一致性和可用性是可以保证的。但分布式是前提，放弃分区意味着放弃了系统的扩展性，不再部署子节点，这违背分布式系统设计的初衷。
	CP：不要求可用，每个请求在服务器直接保持强一致，如果发生网络故障或消息丢失，就要牺牲用户体验，等所有数据同步之后再让用户访问系统。
	AP：不要求一致，在发生故障节点无法同步数据时，每个节点都只能用本地数据提供服务，导致全局数据不一致。

Q: BASE 理论
A: 是对CAP理论的延伸，在分布式系统中，我们往往追求的是可用性，它的重要程序比一致性要高。
	思想是即使无法做到强一致性（CAP的一致性就是强一致性），但可以采用适当的采取弱一致性，即最终一致性。
	BASE是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency）。
	基本可用
			基本可用是指分布式系统在出现故障的时候，允许损失部分可用性（例如响应时间、功能上的可用性），允许损失部分可用性。需要注意的是，基本可用绝不等价于系统不可用。
			响应时间上的损失：正常情况下搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。
			功能上的损失：购物网站在购物高峰（如双十一）时，为了保护系统的稳定性，部分消费者可能会被引导到一个降级页面。
	软状态
			软状态是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据会有多个副本，允许不同副本同步的延时就是软状态的体现。mysql replication的异步复制也是一种体现。
	最终一致性
			最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。			

Q: 分布式项目如何保证单个模块的高可用？
A: 分为 3 个步骤，故障发现、故障消除、故障善后。
	故障发现：
			1. 监控：
					进程外，运用 Zabbix 之类的无探针解决方案，调用系统或者服务自身提供的状态接口获取采集数据。优点无侵入。
									但弊端是监控的粒度太粗，只能进行一些外在的监控。比如可以发现 CPU 突然飙高了，但是并不知道可疑的接口是哪个，更无法知道是哪行代码导致的问题。
					进程内，直接引入采集 SDK 然后硬编码在源代码、通过 AOP 框架来进行动态代码注入。还可以通过利用整个系统中的“连接”部分来进行，比如一些中间件。
									缺点需要侵入到应用程序内部，所以对性能和稳定性会带来一定的影响。
			2.故障注入测试：
					通过技术手段来主动制造“故障”，以此来提前检验系统在各种故障场景下的表现情况是否符合我们预期。
					故障可以被注入到软件，也可以被注入到硬件。
						注入到软件，1. 架设在软件与操作系统之间，当软件中的数据经过操作系统时，通过篡改数据完成注入。
											2. 通过 AOP 之类的框架进行代码注入来制造故障。
						注入到硬件，直接运行一段代码把 CPU、网卡等吃满即可。

	故障消除：
			通过综合运用隔离性、横向扩展、代理、负载均衡、熔断、限流、降级等等机制来快速的“掐灭故障”，防止雪崩的发生。
			
	故障善后：
			故障产生的原因要么是调用的节点处于异常状态，要么是通信链路异常。
			两个进程之间的调用方式。一是直接点对点的同步调用，或者是通过一些技术中间层进行异步的调用。
			1. 同步调用
					1.1 重试，副作用是，如果后端服务总体负载很高，且无法自动弹性扩容，那么会进一步加剧一些压力。
					1.2 将可以容忍最终一致性的同步调用产生的出错消息进行异步重发。比如产生订单的服务有问题，可以先存到消息队列中，再进行异步投递。
			2. 异步调用
					最常见的就是发往消息队列出现异常，通过定时的任务（job）去进行对账（数据一致性检测）。任务（job）的具体实现上尽可能做到自动修正，否则通知人工介入。

Q: 负载均衡的算法有哪些？
A: 常见的负载均衡算法包括轮询法、随机法、加权轮询法、加权随机法、源地址哈希法、一致性哈希法、最小连接法等，应根据具体的使用场景选取对应的算法。

Q: 一致性哈希是什么，虚拟节点？
A: 常用于分布式缓存系统。普通的哈希取余算法，容错性和扩展性不好，如果增加/删除了机器，会有大量数据失效或需要转移。
	一个设计良好的分布式哈希方案应当具有良好的单调性，即服务节点的增减不会造成大量哈希重定位。单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，
			又有新的缓冲加入到系统中，哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。
	
	一致性哈希算法就是这样一种哈希方案。常用于负载均衡。可以取代传统的取模操作。
	一致性哈希将整个哈希值空间组织成一个虚拟的圆环，将服务器按特定的算法计算出其在圆环的位置，对数据key也使用相同算法计算出位置，然后按顺时针
			行走，第一台遇到的服务器就是其定位到的服务器。
	
	容错性与扩展性分析：
			如果一台机器宕机了，原本属于其他机器处理的数据不受影响，该宕机服务器该处理的数据会交到下一台机器。
			如果增加了一台机器，受影响的数据仅仅是其到前一台机器之间的数据，其他不会受影响。
		最大限度抑制了hash键的重新分布。
			
	虚拟节点：
			一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如只有两台机器，一台处理了90%数据，另一台只处理10%。
			所以引入虚拟节点的概念，对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。
			当配置了几百个虚拟节点时，就能抑制分布不均匀。
			
	// TODO 一致性哈希的具体实现

Q: 熔断是什么？实现方式有哪些？
A: 作用类似于保险丝，当某服务出现不可用或响应超时的情况时，为了防止整个系统出现雪崩，暂时停止对该服务的调用。
	可以使用Hystrix提供的熔断和服务降级能力。

Q: 服务降级 fallback
A: 对服务在负荷比较高时，为了预防负荷过载或者响应慢的情况，在其内部暂时舍弃对一些非核心的接口和数据的请求，直接返回一个提前准备好的fallback错误处理信息。
	这样虽然提供的是一个有损的服务，但却保证了整个系统的稳定性和可用性。

Q: 限流算法有哪些？它们的优点和缺点？
A: 常见的限流算法有：计数器、漏桶、令牌桶
	计数器：累计请求数量，超过阈值就拒绝后续的请求。等到计算周期结束后，重置计数。问题是会有突刺现象，短时间内有大量请求直接达到阈值。
	漏桶：   为了消灭突刺现象，可采用漏铜算法。内部有一个队列，请求进来后先放在队列中，另外定期从队列头获取请求并处理。如果队列满了只能丢弃。问题是无法应对短时间突发流量。
	令牌桶：对漏桶算法的改进，能够在限制调用的平均速率的同时，允许一定程度的突发调用。内部有一个队列存放令牌，同时以一定的速率生成令牌放入队列中，
						每次请求都需要获取令牌，否则等待或者拒绝。队列中令牌达到上限也会丢弃，因为它会存有一定数量的令牌，所以允许突发调用。
	都可以使用Guava实现。
	
	集群限流：以上都只能是单机限流，如果在集群中，需要将计数器保存在第三方服务如redis中，每次有相关操作都进行incrementAndGet操作。

Q: 分布式锁
A: 分布式锁需满足四个条件：
		1. 互斥性。在任意时刻，只有一个客户端能持有锁。
		2. 超时自动释放。不会发生死锁。即使有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。
		3. 只有自己才能释放锁。加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给解了，即不能误解锁。
		4. 具有容错性。只要大多数Redis节点正常运行，客户端就能够获取和释放锁。
		
	有三种常见的实现方式：1. 数据库乐观锁、2.基于Redis的分布式锁、3.基于ZooKeeper的分布式锁。
	
	用Redis实现分布式锁（实现一）：使用Jedis
		原子性加锁
			jedis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime);
			通过判断value是否与自身相同可实现重入性

			不使用value=过期时间的原因是 
				1. 需要强制要求分布式下每个客户端时间必须同步
				2.当锁过期的时候，如果多个客户端同时执行jedis.getSet()方法，那么虽然最终只有一个客户端可以加锁，但是这个客户端的锁的过期时间可能被其他客户端覆盖。
				3. 锁不具备拥有者标识，即任何客户端都可以解锁。

		原子性解锁 
			String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";
			Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));
			写了一段lua脚本，保证原子性执行。获取原值判断与requestId是否相同，若相同则解锁。

		该方案满足前三个条件。但是在Redis集群环境下依然存在问题，由于Redis集群数据同步为异步，假设在Master节点获取到锁后未完成数据同步情况下Master节点宕掉，
				主从切换，一个slave节点成为新的Master节点，依然可以获取锁，所以多个Client同时获取到了锁。
		并且过期时间设置不能太短，如果在业务执行完之前锁就过期，会造成多个Client同时获取到了锁。

	用Redis实现分布式锁（实现二）：Redisson RedissonLock
		使用Redisson实现的分布式可重入锁，比自己手动写的效果好，基本原理相同。也都没有解决Master节点挂了之后锁丢失的风险。
		
	用Redis实现分布式锁（实现三）：Redisson RedLock
		解决了单点失败的问题，但代价是需要额外的为 RedissonRedLock 搭建Redis环境。
		// TODO 具体的RedLock原理
		
	用zk实现分布式锁（实现一）
		一个机器接收到了请求之后去 zookeeper 创建一个 znode，接着执行操作；然后另外一个机器也尝试去创建那个 znode，
		结果发现自己创建不了，只能注册监听器监听这个znode。释放锁是删除这个znode，会通知等待的机器。
		
		缺点：所有取锁失败的进程都监听节点，容易发生羊群效应，当释放锁后所有等待进程一起来创建节点，并发量很大。

	用zk实现分布式锁（实现二），有序节点
		上锁改为创建临时有序节点，每个上锁的节点均能创建节点成功，只是其序号不同。只有序号最小的可以拥有锁，
		如果这个节点序号不是最小的则 watch 序号比本身小的前一个节点 (公平锁)。
		
		优点：有效解决单点问题，不可重入问题，非阻塞问题，锁无法释放问题。实现简单。
		缺点：性能没有redis高

Q: 数据库两阶段提交2PC(two - phase - commit)，一致性如何保证？
A: XA Transactions
	第一阶段：事务协调器要求每个涉及到事务的数据库预提交（precommit）此操作，并反映是否可以提交。
	第二阶段：事务协调器要求每个数据库提交数据。
	其中，如果有任何一个数据库否决此次提交，那么所有数据库都会被要求回滚它们在此事务中的那部分信息。
	
	可以在数据库分区之间获得一致性，根据CAP定理，那么一定会影响到可用性。在两阶段提交过程中，要求每个数据库都可用，假设单个数据库可用性为m，
	那么n个数据库整体的可用性就是m的n次方，n越大可用性就会越小

Q: 分布式事务的实现方案有哪些？如果让你自己设计，你应该怎么设计？
A: 两阶段提交（2PC）
		优点：尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。（也不能100%保证强一致）
		缺点：原理简单，牺牲了可用性，对性能影响较大，不适合高并发高性能场景。

	补偿事务（TCC）
		采用的补偿机制，核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿撤销操作。分为三个阶段：
				1. try阶段主要是对业务系统做检测及资源预留。
				2. confirm阶段主要对业务系统做确认提交，try阶段执行成功并开始执行confirm阶段时，默认confirm阶段是不会出错的。要try成功，confirm一定成功。
				3. cancel阶段主要在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。

		举例：假入 Bob 要向 Smith 转账，思路大概是：
					我们有一个本地方法，里面依次调用
					1、首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。
					2、在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。
					3、如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。

		优点： 跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些
		缺点： 入侵业务，更复杂，在2,3步中都有可能失败。TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理。
					如果出现网络连不通怎么办  //TODO

	本地消息表（异步确保）
		业界使用最多。核心思想是将分布式事务拆分成本地事务进行处理。
			消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行重试发送。
			消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败，那么就会重试执行。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。
			生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。
		这种方案遵循BASE理论，采用的是最终一致性，笔者认为是这几种方案里面比较适合实际业务场景的，即不会出现像2PC那样复杂的实现(当调用链很长的时候，2PC的可用性是非常低的)，也不会像TCC那样可能出现确认或者回滚不了的情况。
		
		优点： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。
		缺点： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。

	MQ 事务消息
		RocketMQ支持事务消息，采用的类似二阶段提交方式。但是RabbitMQ和Kafka都不支持。
		具体实现见中间件部分.txt
		
		优点： 实现了最终一致性，不需要依赖本地数据库事务。
		缺点： 实现难度大，主流MQ不支持，RocketMQ事务消息部分代码也未开源。
		
	Sagas 事务模型
		又叫做长时间运行的事务（Long-running-transaction）其核心思想就是拆分分布式系统中的长事务为多个短事务，或者叫多个本地事务，
		然后由 Sagas 工作流引擎负责协调，如果整个流程正常结束，那么就算是业务成功完成，如果在这过程中实现失败，
		那么Sagas工作流引擎就会以相反的顺序调用补偿操作，重新进行业务回滚。
		
		举例：购买旅游套餐业务操作分为三个宝座，预定车辆、预定宾馆、预定机票。从业务上讲他们属于同一个事务。
					假如预定机票失败了，那就要取消宾馆、取消车辆。
					
		这个理论比较新。
		
	你们公司是如何处理分布式事务的？
		特别严格的场景，用的是 TCC 来保证强一致性；然后其他的一些场景基于阿里的 RocketMQ 来实现分布式事务。
	
Q: 故障转移 failover 
A: 当活动的服务或应用意外终止时，快速启用冗余或备用的接替他们工作。

Q: 主备模式
A: 例如Redis的主从架构，实现高可用。

Q: 服务缓存、健康检查、

Q: 做的最好的项目是什么，有什么特色？

Q: 遭遇过哪些印象深刻的困难，最后是怎么解决的？

Q: 如何设计一个高并发系统
A: 首先要结合业务场景设计。总结为6个点
	1. 系统拆分
			将一个系统拆分成多个子系统，微服务，rpc调用，每个系统连一个数据库。
	2. 缓存
			大部分场景是读多写少，数据在缓存里保留一份，读的时候走缓存。Redis可以单机抗几万的并发。
	3. MQ
			消峰，大量写请求可以先进入MQ排队，慢慢被系统消费写入，控制在mysql承载能力之内。即通过MQ来异步写。
	4. 分库分表
			数据库层面抗高并发，可以将一个库拆分成多个库，一个表拆成多个表。
	5. 读写分离
			搞主从架构，主库写入，从库读取。
	6. ElasticSearch
			ES是分布式的，方便扩容，天然可以支撑高并发。对简单的查询、统计、搜索类操作，可以考虑使用ES来做。
			
Q: 为什么要进行系统拆分，如何进行系统拆分。

Q: 分布式服务接口的幂等性如何设计（比如不能重复扣款）？
A: 这个不是技术问题，没有通用的方法，应该结合业务来保证幂等性。
	主要有三点：
			对于每个请求必须有一个唯一的标识，举个栗子：订单支付请求，肯定得包含订单 id，一个订单 id 最多支付一次。
			每次处理完请求之后，必须有一个记录标识这个请求处理过了。订单状态修改成已支付。
			每次接收请求需要进行判断，判断之前是否处理过。判断订单状态。
			
	或者使用分布式锁。

Q: 分布式服务接口请求的顺序性如何保证？
A: 比较困难，一旦引入顺序性保障，比如使用分布式锁，会导致系统复杂度上升，带来效率低下，热点数据过大等问题。
	依靠MQ的顺序消费。
	尽量避免需要顺序的操作，或者将其结合成一个操作。

Q: 集群部署时的分布式 session 如何实现？
A: 完全不用 session
			使用 JWT Token 储存用户身份，然后再从数据库或者 cache 中获取其他的信息。这样无论请求分配到哪个服务器都无所谓。
	tomcat + redis
			基于 tomcat 原生的 session 支持，用一个叫做 Tomcat RedisSessionManager 的东西，
			让所有我们部署的 tomcat 都将 session 数据存储到 redis 即可。但是跟容器耦合。
	spring session + redis
			给 sping session 配置基于 redis 来存储 session 数据，然后配置了一个 spring session 的过滤器，
			这样的话，session 相关操作都会交给 spring session 来管了。接着在代码中，就用原生的 session 操作。
