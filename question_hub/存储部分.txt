大概想法：MySQL、redis、hbase

Q: MySQL的事务隔离级别，有什么问题？丢失更新会发生在什么级别？
A: 读未提交 READ_UNCOMMITTED
	读已提交 READ_COMMITTED
	可重复读 REPEATABLE_READ
	序列化	  SERIALIZABLE
	
	更新丢失类似于不可重复读，因为事务A修改数据过程中，有可能事务B完成了修改，那么事务A提交会覆盖事务B的结果。
		使用隔离级别，设置为可重复读即可。
		使用排它锁，读取数据后不允许其他事务操作。
		使用乐观锁，给数据设置版本号等

Q: MyISAM 和 InnoDB 的区别？
A: 1、MyISAM：默认表类型，它是基于传统的ISAM类型，ISAM是Indexed Sequential Access Method (有索引的顺序访问方法) 的缩写，
			它是存储记录和文件的标准方法。不是事务安全的，而且不支持外键，只支持表锁。如果执行大量的select， MyISAM比较适合。
	2、InnoDB：支持事务安全的引擎，支持外键、行锁、事务是他的最大特点。如果有大量的update和insert，建议使用InnoDB，
			特别是针对多个并发和QPS较高的情况。

Q: MySQL的索引有哪些？区别是什么？
A: 按照索引值的唯一性，可分为唯一索引和非唯一索引。
	按照索引列的个数，可分为单列索引和复合索引。
	按照索引列的物理组织方式，
		B树索引：最常用。// TODO 具体实现
		位图索引：当字段的基数很低时，使用B树效率不高，例如性别只有男女两个值。应当使用位图索引。
		反向键索引：一种特殊的B树，构造相同，但是针对数值会将其翻转再进行索引。适用于自增类型的值。
		基于函数的索引：将要查询用到的表达式作为索引项。
		全局索引和局部索引

Q: MySQL的锁有哪些？意向锁是什么？
A: 大致分为两种锁：读锁（共享锁）和写锁（排它锁）。另外有意向共享锁、意向排它锁、间隙锁。
	意向锁均为表锁，当语句没有使用索引，innodb不能确定操作的行，这时就使用意向锁。
	
Q: 数据库死锁
A: 当两个事务都需要获得对方持有的排他锁才能完成事务，这样就导致了循环锁等待，也就是常见的死锁类型。
	解决死锁的方法：
		1、  数据库参数
		2、  应用中尽量约定程序读取表的顺序一样
		3、  应用中处理一个表时，尽量对处理的顺序排序
		4、  调整事务隔离级别（避免两个事务同时操作一行不存在的数据，容易发生死锁）
	
Q: MySQL表的扩容
A: 数据库很容易成为瓶颈，单机数据库的资源和处理能力有限，在高并发的分布式系统中，可采用分库分表突破单机局限。
	相关内容：
		读写分离: 不同的数据库，同步相同的数据，分别只负责数据的读和写；
		分区: 指定分区列表达式，把记录拆分到不同的区域中(必须是同一服务器，可以是不同硬盘)，应用看来还是同一张表，没有变化；
		分库：一个系统的多张数据表，存储到多个数据库实例中；
		分表: 对于一张多行多列的二维数据表，又分两种情形： 
			(1) 垂直分表: 竖向切分，不同分表存储不同的字段，可以把不常用或者大容量、或者不同业务的字段拆分出去； 
			(2) 水平分表: 横向切分，按照特定分片算法，不同分表存储不同的记录。
	
	分片策略：
		连续分片
			根据特定字段(比如用户ID、订单时间)的范围，值在该区间的，划分到特定节点。 
			优点：集群扩容后，指定新的范围落在新节点即可，无需进行数据迁移。 
			缺点：如果按时间划分，数据热点分布不均(历史数冷，当前数据热)，导致节点负荷不均。
		ID取模分片
			缺点：扩容后需要迁移数据。
		一致性Hash算法
			优点：扩容后只需迁移少量数据。
		Snowflake 分片
			优点：扩容后无需迁移数据。
			
	分库分表带来的问题：
		分布式事务
		跨节点join、连表、聚合等操作
		扩容后，新的分片规则导致迁移数据。

Q: 数据库锁释放时抛异常了，没有释放掉怎么办？
A: 加锁和解锁必须在一个数据库连接里

Q: 数据库在提交事务的时候突然断电，它是如何恢复的？
A: 数据库由两个文件组成，一个数据库文件和一个日志文件。通常情况下日志文件都要比数据库文件大很多，数据库进行任何写入操作的时候都是要先写日志。
	同样在执行事务的时候数据库首先记录下这个事务的redo操作日志，然后才开始真正操作数据库，在操作之前首先会把日志文件写入磁盘，那么当突然断电
	的时候，即使操作没有完成，在重启数据库的时候，会根据当前数据的情况进行undo回滚或者redo前滚，保证数据的强一致性。

Q: 数据库两阶段提交2PC(two - phase - commit)
A: XA Transactions
	第一阶段：事务协调器要求每个涉及到事务的数据库预提交（precommit）此操作，并反映是否可以提交。
	第二阶段：事务协调器要求每个数据库提交数据。
	其中，如果有任何一个数据库否决此次提交，那么所有数据库都会被要求回滚它们在此事务中的那部分信息。
	
	可以在数据库分区之间获得一致性，根据CAP定理，那么一定会影响到可用性。在两阶段提交过程中，要求每个数据库都可用，假设单个数据库可用性为m，
	那么n个数据库整体的可用性就是m的n次方，n越大可用性就会越小

Q: 分布式事务的实现方案有哪些？如果让你自己设计，你应该怎么设计？
A: 两阶段提交（2PC）
		优点：尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。（也不能100%保证强一致）
		缺点：原理简单，牺牲了可用性，对性能影响较大，不适合高并发高性能场景。

	补偿事务（TCC）
		采用的补偿机制，核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿撤销操作。分为三个阶段：
				1. try阶段主要是对业务系统做检测及资源预留。
				2. confirm阶段主要对业务系统做确认提交，try阶段执行成功并开始执行confirm阶段时，默认confirm阶段是不会出错的。要try成功，confirm一定成功。
				3. cancel阶段主要在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。

		举例：假入 Bob 要向 Smith 转账，思路大概是：
					我们有一个本地方法，里面依次调用
					1、首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。
					2、在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。
					3、如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。

		优点： 跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些
		缺点： 入侵业务，更复杂，在2,3步中都有可能失败。TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理。

	本地消息表（异步确保）
		业界使用最多。核心思想是将分布式事务拆分成本地事务进行处理。
			消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行重试发送。
			消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败，那么就会重试执行。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。
			生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。
		这种方案遵循BASE理论，采用的是最终一致性，笔者认为是这几种方案里面比较适合实际业务场景的，即不会出现像2PC那样复杂的实现(当调用链很长的时候，2PC的可用性是非常低的)，也不会像TCC那样可能出现确认或者回滚不了的情况。
		
		优点： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。
		缺点： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。

	MQ 事务消息
		RocketMQ支持事务消息，采用的类似二阶段提交方式。但是RabbitMQ和Kafka都不支持。
		具体实现见中间件部分.txt
		
		优点： 实现了最终一致性，不需要依赖本地数据库事务。
		缺点： 实现难度大，主流MQ不支持，RocketMQ事务消息部分代码也未开源。
		
	Sagas 事务模型
		又叫做长时间运行的事务（Long-running-transaction）其核心思想就是拆分分布式系统中的长事务为多个短事务，或者叫多个本地事务，
		然后由 Sagas 工作流引擎负责协调，如果整个流程正常结束，那么就算是业务成功完成，如果在这过程中实现失败，
		那么Sagas工作流引擎就会以相反的顺序调用补偿操作，重新进行业务回滚。
		
		举例：购买旅游套餐业务操作分为三个宝座，预定车辆、预定宾馆、预定机票。从业务上讲他们属于同一个事务。
					假如预定机票失败了，那就要取消宾馆、取消车辆。
					
		这个理论比较新。
	
Q: Redis、Memcache、MongoDB的区别？
A: 都是NoSQL数据库。Redis、Memcache 比较相似，但与 MongoDB 完全不同，几乎没有可比性。
	Redis/Memcache 是基于内存的，讲究的是性能，多用作缓存层。
	MongoDB 是面向文档的，存储的是类似json的非结构化数据，查询方便，比较像关系型数据库。
	
	数据结构
		M：仅支持key-value类型。value最大只支持1MB
		R：支持k-v、list、set、zset、hash等。value最大支持512MB

	存储方式
		M：把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小 。通过LRU算法抛弃最少使用的数据。
		R：有部分存在硬盘上，支持数据的持久化！突破物理内存的限制。所有的key都在内存中，部分value会交换到磁盘。
		
	底层模型
		M：默认使用Slab Allocation机制管理内存，其主要思想是按照预先规定的大小，将分配的内存分割成特定长度的块以存储相应长度的key-value数据记录，以完全解决内存碎片问题。
		R：自己构建了VM机制，不调用系统函数
		
	分布式可拓展性
		M：在客户端应用中实现，即通过一致性哈希之类的算法指定目标数据的节点。相当于反向代理。
		R：在服务端构建分布式存储，多节点，没有中心节点，各个节点地位一致，可伸缩。节点间通过二进制协议通信，与客户端通过ASCII协议通信。
				每个Master节点都会有对应的两个用于冗余的Slave节点。这样在整个集群中，任意两个节点的宕机都不会导致数据的不可用。
				当Master节点退出后，集群会自动选择一个Slave节点成为新的Master节点。
		
	性能
		M：多核，存储100K以上大数据性能较好。
		R：单核，存储小数据性能较好。可以在服务端直接对数据操作，减少网络IO。
		
	应用场景
		M：动态系统中减轻数据库负载，提升性能；做缓存，适合多读少写，大数据量的情况。
		R：适用于对读写效率要求都很高，数据处理业务复杂和对安全性要求较高的系统。
	
Q: Redis持久化方式有哪些？
A: RDB快照
		Redis支持将当前数据的快照存成一个数据文件的持久化机制，即RDB快照。但是一个持续写入的数据库如何生成快照呢？
		Redis借助了fork命令的copy on write机制。在生成快照时，将当前进程fork出一个子进程，然后在子进程中循环所有的数据，将数据写成为RDB文件。
		我们可以通过Redis的save指令来配置RDB快照生成的时机，比如配置10分钟就生成快照，也可以配置有1000次写入就生成快照，也可以多个规则一起实施。
		这些规则的定义就在Redis的配置文件中，你也可以通过Redis的CONFIG SET命令在Redis运行时设置规则，不需要重启Redis。

		Redis的RDB文件不会马上换掉，因为其写操作是在一个新进程中进行的，当生成一个新的RDB文件时，Redis生成的子进程会先将数据写到一个临时文件中，
		然后通过原子性rename系统调用将临时文件重命名为RDB文件，这样在任何时候出现故障，Redis的RDB文件都总是可用的。
		同时，Redis的RDB文件也是Redis主从同步内部实现中的一环。RDB有他的不足，就是一旦数据库出现问题，那么我们的RDB文件中保存的数据并不是全新的，
		从上次RDB文件生成到Redis停机这段时间的数据全部丢掉了。在某些业务下，这是可以忍受的。

	AOF日志
		全称是append only file，它是一个追加写入的日志文件。类似于binlog，但不同的是，AOF文件是可识别的纯文本，它的内容就是一个个的Redis标准命令。
		只有那些会导致数据发生修改的命令才会追加到AOF文件。每一条修改数据的命令都生成一条日志，AOF文件会越来越大，所以Redis又提供了一个功能，
		叫做AOF rewrite。其功能就是重新生成一份AOF文件，新的AOF文件中一条记录的操作只会有一次，而不像一份老文件那样，可能记录了对同一个值的多次操作。
		其生成过程和RDB类似，也是fork一个进程，直接遍历数据，写入新的AOF临时文件。在写入新文件的过程中，所有的写操作日志还是会写到原来老的AOF文件中，
		同时还会记录在内存缓冲区中。当操作完成后，会将所有缓冲区中的日志一次性写入到临时文件中。然后调用原子性的rename命令用新的AOF文件取代老的AOF文件。

	对于一般性的业务需求，建议使用RDB的方式进行持久化，原因是RDB的开销并相比AOF日志要低很多，对于那些无法忍数据丢失的应用，建议使用AOF日志。

Q: redis服务不可用时，持久化方式有何影响？

Q: redis负载均衡的实现？

Q: redis分布式锁，如果上锁失败怎么办？

Q: 什么是 Redisson ？
A: Redisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。
	它不仅提供了一系列的分布式的Java常用对象，还实现了可重入锁（Reentrant Lock）、公平锁（Fair Lock、联锁（MultiLock）、 红锁（RedLock）、 
	读写锁（ReadWriteLock）等，还提供了许多分布式服务。Redisson提供了使用Redis的最简单和最便捷的方法。
	Redisson的宗旨是促进使用者对Redis的关注分离（Separation of Concern），从而让使用者能够将精力更集中地放在处理业务逻辑上。

Q: 缓存穿透
A: 查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致每次查询不存在的数据都会访问数据库。
	解决方案：
	1. 缓存空值。若查询返回的数据为空，直接缓存一个默认值（比如null），下次查询就有值了。注意，过期时间设置的要较短。
	2. 布隆过滤器。是一个很长的二进制向量和一系列随机映射函数。可以检索一个元素是否在一个集合中。优点是空间效率和查询时间都快，缺点是有一定的误识别率和删除困难。

Q: 缓存击穿
A: 一条数据在缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，在缓存中没有读到数据又同时去数据库查询，
	造成数据库压力过大。例如热点key，重大新闻，微博热搜，而缓存构建需要时间。
	解决方案：
	1. 热点数据缓存永不过期。将过期时间设置在value中。
	2. 加互斥锁，去查数据库时需要先获得互斥锁，查询后设置缓存。要能根据key来加锁，获取A数据不影响其他线程获取B数据。

Q: 缓存雪崩
A: 缓存在一段时间内大量失效，所有查询落在了数据库，造成了缓存雪崩。与缓存击穿不同的是，缓存击穿指并发查同一条数据，
	缓存雪崩是不同数据都过期了，从而查数据库。
	解决方案：
	1. 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。
	2. 使用锁或队列，在缓存失效后，加锁或者队列来控制读数据库或者写缓存的线程数量。
	3. 数据预热，通过缓存的reload机制，预先去更新缓存。
	4. 二级缓存，设置两层缓存，缓存A过期时间较短，缓存B过期时间较长。
	5. 缓存永不过期，不设置过期时间，将过期时间设置在value中。
	
	
	